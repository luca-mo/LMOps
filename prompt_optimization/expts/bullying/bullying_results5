{"task": "bullying", "data_dir": "data/bullying", "prompts": "prompts/bullying.md", "out": "expts/bullying_results5", "max_threads": 8, "temperature": 0.0, "optimizer": "nl-gradient", "rounds": 6, "beam_size": 4, "n_test_exs": 400, "minibatch_size": 64, "n_gradients": 4, "errors_per_gradient": 4, "gradients_per_error": 1, "steps_per_gradient": 1, "mc_samples_per_step": 2, "max_expansion_factor": 8, "engine": "chatgpt", "evaluator": "ucb", "scorer": "01", "eval_rounds": 8, "eval_prompts_per_round": 8, "samples_per_eval": 32, "c": 1.0, "knn_k": 2, "knn_t": 0.993, "reject_on_errors": false, "eval_budget": 2048}
======== ROUND 0
7.987022399902344e-05
('# Task\nIs this tweet cyberbullying?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:',)
(1.0,)
[0.6625]
======== ROUND 1
150.85052609443665
('# Task\nIs there any harmful language or targeted attacks towards an individual or group online in this tweet?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs this tweet promoting hate or harassment towards someone online?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any harmful or abusive language directed towards an individual or group in this tweet?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there harmful or abusive language in this tweet targeting a specific individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.6953125, 0.6770833333333334, 0.67578125, 0.671875)
[0.68, 0.6825, 0.6775, 0.675]
======== ROUND 2
1694.9153988361359
('# Task\nDoes this tweet contain language that could perpetuate harmful stereotypes or contribute to the marginalization of a group or individual?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there language or behavior in this tweet that might be viewed as bullying or harassment towards a particular individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there language in this tweet that could reinforce negative stereotypes or contribute to the marginalization of a group or individual?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or content in this tweet that may be perceived as negative or harmful towards an individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.765625, 0.75, 0.75, 0.75)
[0.695, 0.6725, 0.6925, 0.685]
======== ROUND 3
209.04658198356628
('# Task\nDoes the language used in this tweet perpetuate harmful stereotypes or further marginalize a particular group or individual?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or behavior in this tweet that could be harmful or offensive towards a particular individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language in this tweet that is derogatory or harmful towards a particular individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nDoes this tweet contain language that promotes stereotypes or discriminates against a specific group based on race, gender, sexual orientation, or other characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.7708333333333334, 0.75, 0.75, 0.75)
[0.71, 0.6975, 0.695, 0.695]
======== ROUND 4
209.0389461517334
('# Task\nIs there any language in this tweet that is disrespectful, offensive, or harmful towards a particular individual or group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any content in this tweet that has the potential to harm or marginalize a particular group based on their race, gender, sexual orientation, or other characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nDoes this tweet contain content that could potentially harm or marginalize a specific group based on their race, gender, sexual orientation, or other characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nDoes the language used in this tweet contain derogatory terms or phrases that target a specific group or individual?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.78125, 0.7604166666666666, 0.7604166666666666, 0.75)
[0.7, 0.6975, 0.695, 0.6875]
======== ROUND 5
213.7240288257599
('# Task\nIs there any language or behavior in this tweet that may be seen as discriminatory or offensive towards a particular group due to factors like race, gender, sexual orientation, or other attributes?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or imagery in this tweet that promotes harmful stereotypes or derogatory remarks about a particular group based on their race, gender, sexual orientation, or other characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any content in this tweet that may have the potential to harm or marginalize a particular group of people based on their race, gender, sexual orientation, or other identifying characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or content in this tweet that could be harmful or offensive towards a particular group based on their race, gender, sexual orientation, or other characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.765625, 0.7604166666666666, 0.7604166666666666, 0.75)
[0.7175, 0.6925, 0.7025, 0.7175]
======== ROUND 6
215.48366689682007
('# Task\nIs there any language in this tweet that could be harmful or offensive towards a specific race, gender, sexual orientation, or other characteristic group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or sentiment in this tweet that could contribute to harmful stereotypes or discrimination against a particular group based on their race, gender, sexual orientation, or other identifying characteristics?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language or behavior in the tweet that could be seen as bullying, harassment, or discrimination against a particular group due to their race, gender, sexual orientation, or other traits?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:', '# Task\nIs there any language in this tweet that uses derogatory terms or stereotypes towards a specific race, gender, sexual orientation, or other group?\n\n# Output format\nAnswer Yes or No as labels\n\n# Prediction\nText: {{ text }}\nLabel:')
(0.7708333333333334, 0.7421875, 0.734375, 0.734375)
[0.7, 0.7075, 0.695, 0.67]
